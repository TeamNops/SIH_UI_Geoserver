{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/TeamNops/SIH_UI_Geoserver","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:15:22.639075Z","iopub.execute_input":"2024-12-08T15:15:22.639317Z","iopub.status.idle":"2024-12-08T15:15:25.110834Z","shell.execute_reply.started":"2024-12-08T15:15:22.639275Z","shell.execute_reply":"2024-12-08T15:15:25.109983Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'SIH_UI_Geoserver'...\nremote: Enumerating objects: 164, done.\u001b[K\nremote: Counting objects: 100% (164/164), done.\u001b[K\nremote: Compressing objects: 100% (156/156), done.\u001b[K\nremote: Total 164 (delta 8), reused 163 (delta 7), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (164/164), 31.24 MiB | 48.40 MiB/s, done.\nResolving deltas: 100% (8/8), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os\nfrom typing import Generator, Tuple, List\nfrom glob import glob\n\nclass FILMDataGenerator:\n    def __init__(self, \n                 data_dir: str,\n                 batch_size: int = 8,\n                 image_size: Tuple[int, int] = (256, 256)):\n        \"\"\"\n        Initialize the data generator for FILM model training\n        \n        Args:\n            data_dir: Root directory containing sample folders\n            batch_size: Number of samples per batch\n            image_size: Target size for the images (height, width)\n        \"\"\"\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        \n        # Get all sample directories\n        self.sample_dirs = [d for d in glob(os.path.join(data_dir, \"sample_*\")) \n                          if os.path.isdir(d)]\n        print(f\"Found {len(self.sample_dirs)} sample directories\")\n\n    def load_and_preprocess_image(self, image_path: str) -> tf.Tensor:\n        \"\"\"Load and preprocess a single image\"\"\"\n        # Read image file\n        image = tf.io.read_file(image_path)\n        # Decode image\n        image = tf.io.decode_image(image, channels=3)\n        # Convert to float32 and normalize to [0, 1]\n        image = tf.cast(image, tf.float32) / 255.0\n        # Resize image\n        image = tf.image.resize(image, self.image_size)\n        return image\n\n    def create_dataset(self) -> tf.data.Dataset:\n        \"\"\"Create a TensorFlow dataset for training\"\"\"\n        def generator():\n            while True:\n                # Randomly sample batch_size directories\n                batch_dirs = np.random.choice(self.sample_dirs, \n                                           size=self.batch_size)\n                \n                times = []\n                images_0 = []\n                images_1 = []\n                images_mid = []  # Ground truth middle frames\n                \n                for dir_path in batch_dirs:\n                    # Load the three images\n                    img0 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image.png\"))\n                    img1 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image1.png\"))\n                    img2 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image2.png\"))\n                    \n                    times.append([0.5])  # Middle frame time\n                    images_0.append(img0)\n                    images_1.append(img2)\n                    images_mid.append(img1)\n                \n                # Stack into batches\n                yield {\n                    'time': tf.convert_to_tensor(times, dtype=tf.float32),\n                    'x0': tf.stack(images_0),\n                    'x1': tf.stack(images_1)\n                }, tf.stack(images_mid)\n\n        # Create dataset from generator\n        dataset = tf.data.Dataset.from_generator(\n            generator,\n            output_signature=(\n                {\n                    'time': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n                    'x0': tf.TensorSpec(shape=(None, *self.image_size, 3), \n                                      dtype=tf.float32),\n                    'x1': tf.TensorSpec(shape=(None, *self.image_size, 3), \n                                      dtype=tf.float32)\n                },\n                tf.TensorSpec(shape=(None, *self.image_size, 3), \n                             dtype=tf.float32)\n            )\n        )\n        \n        return dataset.prefetch(tf.data.AUTOTUNE)\n\nclass FILMFineTuner:\n    def __init__(self,\n                 base_model_url: str = \"https://tfhub.dev/google/film/1\",\n                 learning_rate: float = 1e-4):\n        \"\"\"\n        Initialize FILM model fine-tuner\n        \n        Args:\n            base_model_url: URL for the base FILM model\n            learning_rate: Learning rate for training\n        \"\"\"\n        self.model = hub.load(base_model_url)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        \n        # Define loss function (L1 loss)\n        self.loss_fn = tf.keras.losses.MeanAbsoluteError()\n        \n    @tf.function\n    def train_step(self, \n                  inputs: dict,\n                  target: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        \"\"\"\n        Print shapes for debugging\n        \"\"\"\n        tf.print(\"Input shapes:\", \n                \"time:\", inputs['time'].shape,\n                \"x0:\", inputs['x0'].shape,\n                \"x1:\", inputs['x1'].shape,\n                \"target:\", target.shape)\n        \"\"\"\n        Perform one training step\n        \n        Args:\n            inputs: Dictionary containing 'time', 'x0', and 'x1'\n            target: Ground truth middle frame\n            \n        Returns:\n            Tuple of (loss value, predicted frame)\n        \"\"\"\n        with tf.GradientTape() as tape:\n            # Forward pass\n            output_dict = self.model(inputs)\n            # The model returns a dictionary - extract the interpolated frame\n            predicted = output_dict['image']  # Get the interpolated frame from output\n            \n            # Calculate loss\n            loss = self.loss_fn(target, predicted)\n            \n        # Calculate gradients\n        gradients = tape.gradient(loss, self.model.trainable_variables)\n        \n        # Apply gradients\n        self.optimizer.apply_gradients(\n            zip(gradients, self.model.trainable_variables))\n        \n        return loss, predicted\n\ndef main():\n    # Check if dataset directory exists\n    if not os.path.exists(\"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset\"):\n        raise FileNotFoundError(\"interpolation_dataset directory not found\")\n        \n    # Training parameters\n    BATCH_SIZE = 8\n    IMAGE_SIZE = (256, 256)\n    EPOCHS = 10\n    \n    # Initialize data generator\n    data_generator = FILMDataGenerator(\n        data_dir=\"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset\",\n        batch_size=BATCH_SIZE,\n        image_size=IMAGE_SIZE\n    )\n    \n    # Create dataset\n    dataset = data_generator.create_dataset()\n    \n    # Initialize model\n    trainer = FILMFineTuner()\n    \n    # Training loop\n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n        \n        # Initialize metrics\n        epoch_loss = tf.keras.metrics.Mean()\n        \n        for step, (inputs, target) in enumerate(dataset):\n            # Perform training step\n            loss, predicted = trainer.train_step(inputs, target)\n            \n            # Update metrics\n            epoch_loss.update_state(loss)\n            \n            if step % 10 == 0:\n                print(f\"Step {step} - Loss: {epoch_loss.result():.4f}\")\n            \n            # Optional: save checkpoints periodically\n            if step % 1000 == 0:\n                # Save model weights\n                checkpoint_dir = f\"checkpoints/epoch_{epoch}_step_{step}\"\n                os.makedirs(checkpoint_dir, exist_ok=True)\n                 \n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:24:41.699153Z","iopub.execute_input":"2024-12-08T15:24:41.699546Z","iopub.status.idle":"2024-12-08T15:24:55.679601Z","shell.execute_reply.started":"2024-12-08T15:24:41.699512Z","shell.execute_reply":"2024-12-08T15:24:55.678406Z"}},"outputs":[{"name":"stdout","text":"Found 43 sample directories\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os\nfrom typing import Generator, Tuple, List\nfrom glob import glob\n\nclass FILMDataGenerator:\n    def _init_(self, \n                 data_dir: str,\n                 batch_size: int = 8,\n                 image_size: Tuple[int, int] = (256, 256)):\n        \"\"\"\n        Initialize the data generator for FILM model training\n        \n        Args:\n            data_dir: Root directory containing sample folders\n            batch_size: Number of samples per batch\n            image_size: Target size for the images (height, width)\n        \"\"\"\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        \n        # Get all sample directories\n        self.sample_dirs = [d for d in glob(os.path.join(data_dir, \"sample_*\")) \n                            if os.path.isdir(d)]\n        print(f\"Found {len(self.sample_dirs)} sample directories\")\n\n    def load_and_preprocess_image(self, image_path: str) -> tf.Tensor:\n        \"\"\"Load and preprocess a single image\"\"\"\n        # Read image file\n        image = tf.io.read_file(image_path)\n        # Decode image\n        image = tf.io.decode_image(image, channels=3)\n        # Convert to float32 and normalize to [0, 1]\n        image = tf.cast(image, tf.float32) / 255.0\n        # Resize image\n        image = tf.image.resize(image, self.image_size)\n        return image\n\n    def create_dataset(self) -> tf.data.Dataset:\n        \"\"\"Create a TensorFlow dataset for training\"\"\"\n        def generator():\n            while True:\n                # Randomly sample batch_size directories\n                batch_dirs = np.random.choice(self.sample_dirs, \n                                              size=self.batch_size)\n                \n                times = []\n                images_0 = []\n                images_1 = []\n                images_mid = []  # Ground truth middle frames\n                \n                for dir_path in batch_dirs:\n                    # Load the three images\n                    img0 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image.png\"))\n                    img1 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image1.png\"))\n                    img2 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image2.png\"))\n                    \n                    times.append([0.5])  # Middle frame time\n                    images_0.append(img0)\n                    images_1.append(img2)\n                    images_mid.append(img1)\n                \n                # Stack into batches\n                yield {\n                    'time': tf.convert_to_tensor(times, dtype=tf.float32),\n                    'x0': tf.stack(images_0),\n                    'x1': tf.stack(images_1)\n                }, tf.stack(images_mid)\n\n        # Create dataset from generator\n        dataset = tf.data.Dataset.from_generator(\n            generator,\n            output_signature=(\n                {\n                    'time': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n                    'x0': tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32),\n                    'x1': tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32)\n                },\n                tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32)\n            )\n        )\n        \n        # Prefetch for performance\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset\n\nclass FILMFineTuner:\n    def _init_(self,\n                 base_model_url: str = \"https://tfhub.dev/google/film/1\",\n                 learning_rate: float = 1e-4):\n        \"\"\"\n        Initialize FILM model fine-tuner\n        \n        Args:\n            base_model_url: URL for the base FILM model\n            learning_rate: Learning rate for training\n        \"\"\"\n        self.model = hub.load(base_model_url)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        \n        # Define loss function (L1 loss)\n        self.loss_fn = tf.keras.losses.MeanAbsoluteError()\n        \n    @tf.function\n    def train_step(self, inputs: dict, target: tf.Tensor):\n        \"\"\"\n        Perform one training step\n        \n        Args:\n            inputs: Dictionary containing 'time', 'x0', and 'x1'\n            target: Ground truth middle frame\n            \n        Returns:\n            loss value, predicted frame\n        \"\"\"\n        with tf.GradientTape() as tape:\n            output_dict = self.model(inputs)\n            predicted = output_dict['image']  # interpolated frame\n            loss = self.loss_fn(target, predicted)\n\n        gradients = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.model.trainable_variables))\n        \n        return loss, predicted\n\ndef main():\n    # Check if dataset directory exists\n    data_dir = \"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset\"\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(\"interpolation_dataset directory not found\")\n        \n    # Training parameters\n    BATCH_SIZE = 8\n    IMAGE_SIZE = (256, 256)\n    EPOCHS = 10\n\n    # Create the MirroredStrategy for multi-GPU training\n    strategy = tf.distribute.MirroredStrategy()\n    print(\"Number of devices: \", strategy.num_replicas_in_sync)\n\n    # Initialize data generator\n    data_generator = FILMDataGenerator(\n        data_dir=data_dir,\n        batch_size=BATCH_SIZE,\n        image_size=IMAGE_SIZE\n    )\n    \n    # Create dataset\n    dataset = data_generator.create_dataset()\n    \n    # Distribute the dataset\n    # Each batch returned will be split across the available GPUs.\n    dataset = strategy.experimental_distribute_dataset(dataset)\n    \n    with strategy.scope():\n        # Initialize model (done under strategy scope)\n        trainer = FILMFineTuner()\n\n        # Custom training loop across epochs\n        for epoch in range(EPOCHS):\n            print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n            \n            epoch_loss = tf.keras.metrics.Mean()\n\n            # Define a distributed train step to run on each replica\n            @tf.function\n            def distributed_train_step(dist_inputs, dist_target):\n                per_replica_losses, _ = strategy.run(\n                    trainer.train_step, \n                    args=(dist_inputs, dist_target)\n                )\n                # per_replica_losses is a per-replica tensor, need to reduce it\n                mean_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)\n                return mean_loss\n            \n            for step, (inputs, target) in enumerate(dataset):\n                # One train step\n                loss = distributed_train_step(inputs, target)\n                epoch_loss.update_state(loss)\n\n                if step % 10 == 0:\n                    print(f\"Step {step} - Loss: {epoch_loss.result():.4f}\")\n                \n                # Optional checkpoint saving (not fully implemented)\n                if step % 1000 == 0 and step > 0:\n                    checkpoint_dir = f\"checkpoints/epoch_{epoch}_step{step}\"\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    # Here you would implement saving logic for model variables if desired.\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T15:25:16.009637Z","iopub.execute_input":"2024-12-08T15:25:16.009982Z","iopub.status.idle":"2024-12-08T15:25:16.423130Z","shell.execute_reply.started":"2024-12-08T15:25:16.009951Z","shell.execute_reply":"2024-12-08T15:25:16.421877Z"}},"outputs":[{"name":"stdout","text":"Number of devices:  2\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 198\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[38;5;66;03m# Here you would implement saving logic for model variables if desired.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 149\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of devices: \u001b[39m\u001b[38;5;124m\"\u001b[39m, strategy\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Initialize data generator\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m data_generator \u001b[38;5;241m=\u001b[39m \u001b[43mFILMDataGenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_SIZE\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[1;32m    156\u001b[0m dataset \u001b[38;5;241m=\u001b[39m data_generator\u001b[38;5;241m.\u001b[39mcreate_dataset()\n","\u001b[0;31mTypeError\u001b[0m: FILMDataGenerator() takes no arguments"],"ename":"TypeError","evalue":"FILMDataGenerator() takes no arguments","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os\nfrom typing import Generator, Tuple, List\nfrom glob import glob\nclass FILMDataGenerator:\n    def __init__(self, \n                 data_dir: str,\n                 batch_size: int = 8,\n                 image_size: Tuple[int, int] = (256, 256)):\n        \"\"\"\n        Initialize the data generator for FILM model training\n        \n        Args:\n            data_dir: Root directory containing sample folders\n            batch_size: Number of samples per batch\n            image_size: Target size for the images (height, width)\n        \"\"\"\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        \n        # Get all sample directories\n        self.sample_dirs = [d for d in glob(os.path.join(data_dir, \"sample_*\")) \n                            if os.path.isdir(d)]\n        print(f\"Found {len(self.sample_dirs)} sample directories\")\n        \n        # Calculate steps per epoch\n        self.steps_per_epoch = len(self.sample_dirs) // self.batch_size\n        if len(self.sample_dirs) % self.batch_size != 0:\n            self.steps_per_epoch += 1\n        print(f\"Steps per epoch: {self.steps_per_epoch}\")\n\n    def load_and_preprocess_image(self, image_path: str) -> tf.Tensor:\n        \"\"\"Load and preprocess a single image\"\"\"\n        # Read image file\n        image = tf.io.read_file(image_path)\n        # Decode image\n        image = tf.io.decode_image(image, channels=3)\n        # Convert to float32 and normalize to [0, 1]\n        image = tf.cast(image, tf.float32) / 255.0\n        # Resize image\n        image = tf.image.resize(image, self.image_size)\n        return image\n\n    def create_dataset(self) -> tf.data.Dataset:\n        \"\"\"Create a TensorFlow dataset for training\"\"\"\n        def generator():\n            # Shuffle the sample directories\n            indices = np.arange(len(self.sample_dirs))\n            np.random.shuffle(indices)\n            \n            # Process in batches\n            for i in range(0, len(indices), self.batch_size):\n                batch_indices = indices[i:i + self.batch_size]\n                batch_dirs = [self.sample_dirs[idx] for idx in batch_indices]\n                \n                times = []\n                images_0 = []\n                images_1 = []\n                images_mid = []\n                \n                for dir_path in batch_dirs:\n                    # Load the three images\n                    img0 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image.png\"))\n                    img1 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image1.png\"))\n                    img2 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image2.png\"))\n                    \n                    times.append([0.5])\n                    images_0.append(img0)\n                    images_1.append(img2)\n                    images_mid.append(img1)\n                \n                # Stack into batches\n                yield {\n                    'time': tf.convert_to_tensor(times, dtype=tf.float32),\n                    'x0': tf.stack(images_0),\n                    'x1': tf.stack(images_1)\n                }, tf.stack(images_mid)\n\n        # Create dataset from generator\n        dataset = tf.data.Dataset.from_generator(\n            generator,\n            output_signature=(\n                {\n                    'time': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n                    'x0': tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32),\n                    'x1': tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32)\n                },\n                tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32)\n            )\n        )\n        \n        return dataset.prefetch(tf.data.AUTOTUNE)\n\nclass FILMFineTuner:\n    def __init__(self,\n                 base_model_url: str = \"https://tfhub.dev/google/film/1\",\n                 learning_rate: float = 1e-4):\n        \"\"\"\n        Initialize FILM model fine-tuner\n        \n        Args:\n            base_model_url: URL for the base FILM model\n            learning_rate: Learning rate for training\n        \"\"\"\n        self.model = hub.load(base_model_url)  # Properly load the model\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        \n        # Define loss function (L1 loss)\n        self.loss_fn = tf.keras.losses.MeanAbsoluteError()\n\n        \n    @tf.function\n    def train_step(self, inputs: dict, target: tf.Tensor):\n        \"\"\"\n        Perform one training step\n        \n        Args:\n            inputs: Dictionary containing 'time', 'x0', and 'x1'\n            target: Ground truth middle frame\n            \n        Returns:\n            loss value, predicted frame\n        \"\"\"\n        with tf.GradientTape() as tape:\n            output_dict = self.model(inputs)\n            predicted = output_dict['image']  # interpolated frame\n            loss = self.loss_fn(target, predicted)\n\n        gradients = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.model.trainable_variables))\n        \n        return loss, predicted\n\ndef main():\n    # Check if dataset directory exists\n    data_dir = \"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset\"\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(\"interpolation_dataset directory not found\")\n        \n    # Training parameters\n    BATCH_SIZE = 8\n    IMAGE_SIZE = (256, 256)\n    EPOCHS = 10\n\n    # Initialize data generator\n    data_generator = FILMDataGenerator(\n        data_dir=data_dir,\n        batch_size=BATCH_SIZE,\n        image_size=IMAGE_SIZE\n    )\n    \n    # Create strategy\n    strategy = tf.distribute.MirroredStrategy()\n    print(\"Number of devices: \", strategy.num_replicas_in_sync)\n    \n    with strategy.scope():\n        # Initialize model\n        trainer = FILMFineTuner()\n\n        # Training loop\n        for epoch in range(EPOCHS):\n            print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n            \n            # Create new dataset for this epoch\n            dataset = data_generator.create_dataset()\n            dist_dataset = strategy.experimental_distribute_dataset(dataset)\n            \n            epoch_loss = tf.keras.metrics.Mean()\n            \n            @tf.function\n            def distributed_train_step(dist_inputs, dist_target):\n                per_replica_losses, _ = strategy.run(\n                    trainer.train_step, \n                    args=(dist_inputs, dist_target)\n                )\n                return strategy.reduce(tf.distribute.ReduceOp.MEAN, \n                                     per_replica_losses, \n                                     axis=None)\n            \n            # Train for exactly one epoch\n            for step, (inputs, target) in enumerate(dist_dataset):\n                if step >= data_generator.steps_per_epoch:\n                    break\n                    \n                loss = distributed_train_step(inputs, target)\n                epoch_loss.update_state(loss)\n                \n                print(f\"Step {step + 1}/{data_generator.steps_per_epoch} - \"\n                      f\"Loss: {epoch_loss.result():.4f}\")\n                \n                # Optional checkpoint saving\n                if (step + 1) % 5 == 0:  # Adjust frequency as needed\n                    checkpoint_dir = f\"checkpoints/epoch_{epoch}_step{step}\"\n                    os.makedirs(checkpoint_dir, exist_ok=True)\n                    # Implement saving logic here if desired\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:02:41.833317Z","iopub.execute_input":"2024-12-08T16:02:41.833707Z","iopub.status.idle":"2024-12-08T16:12:49.990443Z","shell.execute_reply.started":"2024-12-08T16:02:41.833677Z","shell.execute_reply":"2024-12-08T16:12:49.989714Z"}},"outputs":[{"name":"stdout","text":"Found 43 sample directories\nSteps per epoch: 6\nNumber of devices:  2\n\nEpoch 1/10\nStep 1/6 - Loss: 0.1276\nStep 2/6 - Loss: 0.1155\nStep 3/6 - Loss: 0.1168\nStep 4/6 - Loss: 0.1171\nStep 5/6 - Loss: 0.1168\nStep 6/6 - Loss: 0.1082\n\nEpoch 2/10\nStep 1/6 - Loss: 0.1102\nStep 2/6 - Loss: 0.1128\nStep 3/6 - Loss: 0.1095\nStep 4/6 - Loss: 0.1087\nStep 5/6 - Loss: 0.1092\nStep 6/6 - Loss: 0.1063\n\nEpoch 3/10\nStep 1/6 - Loss: 0.1060\nStep 2/6 - Loss: 0.1038\nStep 3/6 - Loss: 0.1000\nStep 4/6 - Loss: 0.1053\nStep 5/6 - Loss: 0.1089\nStep 6/6 - Loss: 0.1042\n\nEpoch 4/10\nStep 1/6 - Loss: 0.1211\nStep 2/6 - Loss: 0.1079\nStep 3/6 - Loss: 0.1085\nStep 4/6 - Loss: 0.1099\nStep 5/6 - Loss: 0.1059\nStep 6/6 - Loss: 0.1090\n\nEpoch 5/10\nStep 1/6 - Loss: 0.0968\nStep 2/6 - Loss: 0.1038\nStep 3/6 - Loss: 0.1076\nStep 4/6 - Loss: 0.1052\nStep 5/6 - Loss: 0.1076\nStep 6/6 - Loss: 0.1061\n\nEpoch 6/10\nStep 1/6 - Loss: 0.1035\nStep 2/6 - Loss: 0.1012\nStep 3/6 - Loss: 0.1018\nStep 4/6 - Loss: 0.1071\nStep 5/6 - Loss: 0.1039\nStep 6/6 - Loss: 0.1060\n\nEpoch 7/10\nStep 1/6 - Loss: 0.1066\nStep 2/6 - Loss: 0.0972\nStep 3/6 - Loss: 0.0977\nStep 4/6 - Loss: 0.0998\nStep 5/6 - Loss: 0.1024\nStep 6/6 - Loss: 0.1030\n\nEpoch 8/10\nStep 1/6 - Loss: 0.1180\nStep 2/6 - Loss: 0.1133\nStep 3/6 - Loss: 0.1066\nStep 4/6 - Loss: 0.1034\nStep 5/6 - Loss: 0.1043\nStep 6/6 - Loss: 0.1066\n\nEpoch 9/10\nStep 1/6 - Loss: 0.1127\nStep 2/6 - Loss: 0.1131\nStep 3/6 - Loss: 0.1045\nStep 4/6 - Loss: 0.0997\nStep 5/6 - Loss: 0.1012\nStep 6/6 - Loss: 0.1058\n\nEpoch 10/10\nStep 1/6 - Loss: 0.0943\nStep 2/6 - Loss: 0.0957\nStep 3/6 - Loss: 0.0965\nStep 4/6 - Loss: 0.0986\nStep 5/6 - Loss: 0.1004\nStep 6/6 - Loss: 0.1026\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Ensure 'trainer' is defined and 'trainer.model' contains your trained model.\nfinal_model_save_path = 'Models/final_model'\ntrainer.model.save(final_model_save_path)  # Save the Keras model\nprint(f\"Final model saved at {final_model_save_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:14:01.757653Z","iopub.execute_input":"2024-12-08T16:14:01.758534Z","iopub.status.idle":"2024-12-08T16:14:01.785922Z","shell.execute_reply.started":"2024-12-08T16:14:01.758498Z","shell.execute_reply":"2024-12-08T16:14:01.784751Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ensure 'trainer' is defined and 'trainer.model' contains your trained model.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m final_model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModels/final_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(final_model_save_path)  \u001b[38;5;66;03m# Save the Keras model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal model saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"SAVE for EACH EPOCH","metadata":{}},{"cell_type":"code","source":"class FILMFineTuner:\n    def __init__(self,\n                 base_model_url: str = \"https://tfhub.dev/google/film/1\",\n                 learning_rate: float = 1e-4):\n        \"\"\"\n        Initialize FILM model fine-tuner\n        \n        Args:\n            base_model_url: URL for the base FILM model\n            learning_rate: Learning rate for training\n        \"\"\"\n        self.model = hub.load(base_model_url)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        self.loss_fn = tf.keras.losses.MeanAbsoluteError()\n        \n        # Initialize checkpoint manager\n        self.checkpoint = tf.train.Checkpoint(\n            optimizer=self.optimizer,\n            model=self.model,\n            epoch=tf.Variable(0)\n        )\n        \n    def save_model(self, save_dir: str, epoch: int = None):\n        \"\"\"\n        Save the model weights and optimizer state\n        \n        Args:\n            save_dir: Directory to save the model\n            epoch: Current epoch number (optional)\n        \"\"\"\n        if epoch is not None:\n            self.checkpoint.epoch.assign(epoch)\n            \n        # Create save directory if it doesn't exist\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Save the model weights\n        weights_path = os.path.join(save_dir, 'model_weights')\n        self.model.save_weights(weights_path)\n        \n        # Save optimizer state and epoch\n        checkpoint_path = os.path.join(save_dir, 'checkpoint')\n        self.checkpoint.save(checkpoint_path)\n        \n        print(f\"Model saved successfully to {save_dir}\")\n        \n    def load_model(self, save_dir: str):\n        \"\"\"\n        Load the model weights and optimizer state\n        \n        Args:\n            save_dir: Directory containing the saved model\n        \"\"\"\n        # Load model weights\n        weights_path = os.path.join(save_dir, 'model_weights')\n        self.model.load_weights(weights_path)\n        \n        # Load optimizer state and epoch\n        checkpoint_path = os.path.join(save_dir, 'checkpoint')\n        self.checkpoint.restore(tf.train.latest_checkpoint(save_dir))\n        \n        print(f\"Model loaded successfully from {save_dir}\")\n        return self.checkpoint.epoch.numpy()  # Return the loaded epoch number\n\ndef main():\n    # ... (previous code remains the same until the training loop)\n    \n    # Training parameters\n    BATCH_SIZE = 8\n    IMAGE_SIZE = (256, 256)\n    EPOCHS = 10\n    SAVE_DIR = \"saved_model\"  # Directory to save the model\n    \n    with strategy.scope():\n        # Initialize model\n        trainer = FILMFineTuner()\n        \n        # Training loop\n        for epoch in range(EPOCHS):\n            print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n            \n            # Create new dataset for this epoch\n            dataset = data_generator.create_dataset()\n            dist_dataset = strategy.experimental_distribute_dataset(dataset)\n            \n            epoch_loss = tf.keras.metrics.Mean()\n            \n            # ... (training steps remain the same)\n            \n            # Save model at the end of each epoch\n            if (epoch + 1) % 1 == 0:  # Save every epoch (adjust frequency as needed)\n                epoch_save_dir = os.path.join(SAVE_DIR, f'epoch_{epoch + 1}')\n                trainer.save_model(epoch_save_dir, epoch + 1)\n                \n                # Save a complete model at the final epoch\n                if epoch + 1 == EPOCHS:\n                    final_save_dir = os.path.join(SAVE_DIR, 'final_model')\n                    trainer.save_model(final_save_dir)\n\n# To load and use the saved model later:\ndef load_and_use_model(save_dir: str, image1_path: str, image2_path: str, time: float = 0.5):\n    \"\"\"\n    Load the saved model and use it for frame interpolation\n    \n    Args:\n        save_dir: Directory containing the saved model\n        image1_path: Path to first input image\n        image2_path: Path to second input image\n        time: Time step for interpolation (between 0 and 1)\n    \n    Returns:\n        Interpolated frame\n    \"\"\"\n    # Initialize model\n    trainer = FILMFineTuner()\n    \n    # Load saved weights\n    epoch = trainer.load_model(save_dir)\n    print(f\"Loaded model from epoch {epoch}\")\n    \n    # Preprocess images\n    def load_image(path):\n        image = tf.io.read_file(path)\n        image = tf.io.decode_image(image, channels=3)\n        image = tf.cast(image, tf.float32) / 255.0\n        image = tf.image.resize(image, (256, 256))\n        return image\n    \n    img1 = load_image(image1_path)\n    img2 = load_image(image2_path)\n    \n    # Prepare input\n    inputs = {\n        'time': tf.convert_to_tensor([[time]], dtype=tf.float32),\n        'x0': tf.expand_dims(img1, 0),\n        'x1': tf.expand_dims(img2, 0)\n    }\n    \n    # Generate interpolated frame\n    output = trainer.model(inputs)\n    interpolated_frame = output['image'][0]  # Remove batch dimension\n    \n    return interpolated_frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os\nfrom typing import Generator, Tuple, List\nfrom glob import glob\n\nclass FILMDataGenerator:\n    def __init__(self, \n                 data_dir: str,\n                 batch_size: int = 8,\n                 image_size: Tuple[int, int] = (256, 256)):\n        \"\"\"\n        Initialize the data generator for FILM model training\n        \n        Args:\n            data_dir: Root directory containing sample folders\n            batch_size: Number of samples per batch\n            image_size: Target size for the images (height, width)\n        \"\"\"\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.image_size = image_size\n        \n        # Get all sample directories\n        self.sample_dirs = [d for d in glob(os.path.join(data_dir, \"sample_*\")) \n                            if os.path.isdir(d)]\n        print(f\"Found {len(self.sample_dirs)} sample directories\")\n        \n        # Calculate steps per epoch\n        self.steps_per_epoch = len(self.sample_dirs) // self.batch_size\n        if len(self.sample_dirs) % self.batch_size != 0:\n            self.steps_per_epoch += 1\n        print(f\"Steps per epoch: {self.steps_per_epoch}\")\n\n    def load_and_preprocess_image(self, image_path: str) -> tf.Tensor:\n        \"\"\"Load and preprocess a single image\"\"\"\n        # Read image file\n        image = tf.io.read_file(image_path)\n        # Decode image\n        image = tf.io.decode_image(image, channels=3)\n        # Convert to float32 and normalize to [0, 1]\n        image = tf.cast(image, tf.float32) / 255.0\n        # Resize image\n        image = tf.image.resize(image, self.image_size)\n        return image\n\n    def create_dataset(self) -> tf.data.Dataset:\n        \"\"\"Create a TensorFlow dataset for training\"\"\"\n        def generator():\n            # Shuffle the sample directories\n            indices = np.arange(len(self.sample_dirs))\n            np.random.shuffle(indices)\n            \n            # Process in batches\n            for i in range(0, len(indices), self.batch_size):\n                batch_indices = indices[i:i + self.batch_size]\n                batch_dirs = [self.sample_dirs[idx] for idx in batch_indices]\n                \n                times = []\n                images_0 = []\n                images_1 = []\n                images_mid = []\n                \n                for dir_path in batch_dirs:\n                    # Load the three images\n                    img0 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image.png\"))\n                    img1 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image1.png\"))\n                    img2 = self.load_and_preprocess_image(\n                        os.path.join(dir_path, \"image2.png\"))\n                    \n                    times.append([0.5])\n                    images_0.append(img0)\n                    images_1.append(img2)\n                    images_mid.append(img1)\n                \n                # Stack into batches\n                yield {\n                    'time': tf.convert_to_tensor(times, dtype=tf.float32),\n                    'x0': tf.stack(images_0),\n                    'x1': tf.stack(images_1)\n                }, tf.stack(images_mid)\n\n        # Create dataset from generator\n        dataset = tf.data.Dataset.from_generator(\n            generator,\n            output_signature=(\n                {\n                    'time': tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n                    'x0': tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32),\n                    'x1': tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32)\n                },\n                tf.TensorSpec(shape=(None, *self.image_size, 3), dtype=tf.float32)\n            )\n        )\n        \n        return dataset.prefetch(tf.data.AUTOTUNE)\n\nclass FILMFineTuner:\n    def __init__(self,\n                 base_model_url: str = \"https://tfhub.dev/google/film/1\",\n                 learning_rate: float = 1e-4,\n                 checkpoint_dir: str = \"model_checkpoints\"):\n        \"\"\"\n        Initialize FILM model fine-tuner\n        \n        Args:\n            base_model_url: URL for the base FILM model\n            learning_rate: Learning rate for training\n            checkpoint_dir: Directory to save model checkpoints\n        \"\"\"\n        self.model = hub.load(base_model_url)\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n        self.loss_fn = tf.keras.losses.MeanAbsoluteError()\n        self.checkpoint_dir = checkpoint_dir\n        \n        # Create checkpoint manager\n        self.checkpoint = tf.train.Checkpoint(\n            optimizer=self.optimizer,\n            model=self.model\n        )\n        self.manager = tf.train.CheckpointManager(\n            self.checkpoint,\n            self.checkpoint_dir,\n            max_to_keep=5  # Keep last 5 checkpoints\n        )\n        \n    def save_checkpoint(self, epoch):\n        \"\"\"Save model checkpoint\"\"\"\n        path = self.manager.save(checkpoint_number=epoch)\n        return path\n        \n    def restore_latest_checkpoint(self):\n        \"\"\"Restore the latest checkpoint if it exists\"\"\"\n        if self.manager.latest_checkpoint:\n            self.checkpoint.restore(self.manager.latest_checkpoint)\n            print(f\"Restored from checkpoint: {self.manager.latest_checkpoint}\")\n            return True\n        return False\n\n    @tf.function\n    def train_step(self, inputs: dict, target: tf.Tensor):\n        \"\"\"Train step implementation remains the same\"\"\"\n        with tf.GradientTape() as tape:\n            output_dict = self.model(inputs)\n            predicted = output_dict['image']\n            loss = self.loss_fn(target, predicted)\n\n        gradients = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.model.trainable_variables))\n        \n        return loss, predicted\n\ndef main():\n    # Check if dataset directory exists\n    data_dir = \"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset\"\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(\"interpolation_dataset directory not found\")\n        \n    # Training parameters\n    BATCH_SIZE = 8\n    IMAGE_SIZE = (256, 256)\n    EPOCHS = 20  # Changed to 20 epochs\n    CHECKPOINT_DIR = \"film_model_checkpoints\"\n    \n    # Create checkpoint directory\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n    # Initialize data generator\n    data_generator = FILMDataGenerator(\n        data_dir=data_dir,\n        batch_size=BATCH_SIZE,\n        image_size=IMAGE_SIZE\n    )\n    \n    # Create strategy\n    strategy = tf.distribute.MirroredStrategy()\n    print(\"Number of devices: \", strategy.num_replicas_in_sync)\n    \n    with strategy.scope():\n        # Initialize model with checkpoint directory\n        trainer = FILMFineTuner(checkpoint_dir=CHECKPOINT_DIR)\n        \n        # Attempt to restore from checkpoint\n        start_epoch = 0\n        if trainer.restore_latest_checkpoint():\n            # Extract the epoch number from checkpoint path\n            checkpoint_path = trainer.manager.latest_checkpoint\n            if checkpoint_path:\n                start_epoch = int(checkpoint_path.split('ckpt-')[-1]) + 1\n                print(f\"Starting from epoch {start_epoch}\")\n\n        # Training loop\n        for epoch in range(start_epoch, EPOCHS):\n            print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n            \n            # Create new dataset for this epoch\n            dataset = data_generator.create_dataset()\n            dist_dataset = strategy.experimental_distribute_dataset(dataset)\n            \n            epoch_loss = tf.keras.metrics.Mean()\n            \n            @tf.function\n            def distributed_train_step(dist_inputs, dist_target):\n                per_replica_losses, _ = strategy.run(\n                    trainer.train_step, \n                    args=(dist_inputs, dist_target)\n                )\n                return strategy.reduce(tf.distribute.ReduceOp.MEAN, \n                                     per_replica_losses, \n                                     axis=None)\n            \n            # Train for one epoch\n            for step, (inputs, target) in enumerate(dist_dataset):\n                if step >= data_generator.steps_per_epoch:\n                    break\n                    \n                loss = distributed_train_step(inputs, target)\n                epoch_loss.update_state(loss)\n                \n                if (step + 1) % 10 == 0:  # Print every 10 steps\n                    print(f\"Step {step + 1}/{data_generator.steps_per_epoch} - \"\n                          f\"Loss: {epoch_loss.result():.4f}\")\n            \n            # Save checkpoint after each epoch\n            checkpoint_path = trainer.save_checkpoint(epoch)\n            print(f\"\\nEpoch {epoch + 1} completed. \"\n                  f\"Average loss: {epoch_loss.result():.4f}\")\n            print(f\"Checkpoint saved: {checkpoint_path}\")\n            \n            # Save training metrics\n            with open(os.path.join(CHECKPOINT_DIR, 'training_log.txt'), 'a') as f:\n                f.write(f\"Epoch {epoch + 1}, Loss: {epoch_loss.result():.4f}\\n\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:18:03.614640Z","iopub.execute_input":"2024-12-08T16:18:03.615062Z","iopub.status.idle":"2024-12-08T16:38:01.987846Z","shell.execute_reply.started":"2024-12-08T16:18:03.615028Z","shell.execute_reply":"2024-12-08T16:38:01.987039Z"}},"outputs":[{"name":"stdout","text":"Found 43 sample directories\nSteps per epoch: 6\nNumber of devices:  2\n\nEpoch 1/20\n\nEpoch 1 completed. Average loss: 0.1134\nCheckpoint saved: film_model_checkpoints/ckpt-0\n\nEpoch 2/20\n\nEpoch 2 completed. Average loss: 0.1085\nCheckpoint saved: film_model_checkpoints/ckpt-1\n\nEpoch 3/20\n\nEpoch 3 completed. Average loss: 0.1066\nCheckpoint saved: film_model_checkpoints/ckpt-2\n\nEpoch 4/20\n\nEpoch 4 completed. Average loss: 0.1023\nCheckpoint saved: film_model_checkpoints/ckpt-3\n\nEpoch 5/20\n\nEpoch 5 completed. Average loss: 0.1090\nCheckpoint saved: film_model_checkpoints/ckpt-4\n\nEpoch 6/20\n\nEpoch 6 completed. Average loss: 0.1033\nCheckpoint saved: film_model_checkpoints/ckpt-5\n\nEpoch 7/20\n\nEpoch 7 completed. Average loss: 0.1053\nCheckpoint saved: film_model_checkpoints/ckpt-6\n\nEpoch 8/20\n\nEpoch 8 completed. Average loss: 0.1034\nCheckpoint saved: film_model_checkpoints/ckpt-7\n\nEpoch 9/20\n\nEpoch 9 completed. Average loss: 0.1061\nCheckpoint saved: film_model_checkpoints/ckpt-8\n\nEpoch 10/20\n\nEpoch 10 completed. Average loss: 0.1071\nCheckpoint saved: film_model_checkpoints/ckpt-9\n\nEpoch 11/20\n\nEpoch 11 completed. Average loss: 0.1040\nCheckpoint saved: film_model_checkpoints/ckpt-10\n\nEpoch 12/20\n\nEpoch 12 completed. Average loss: 0.1026\nCheckpoint saved: film_model_checkpoints/ckpt-11\n\nEpoch 13/20\n\nEpoch 13 completed. Average loss: 0.1057\nCheckpoint saved: film_model_checkpoints/ckpt-12\n\nEpoch 14/20\n\nEpoch 14 completed. Average loss: 0.1046\nCheckpoint saved: film_model_checkpoints/ckpt-13\n\nEpoch 15/20\n\nEpoch 15 completed. Average loss: 0.0999\nCheckpoint saved: film_model_checkpoints/ckpt-14\n\nEpoch 16/20\n\nEpoch 16 completed. Average loss: 0.1002\nCheckpoint saved: film_model_checkpoints/ckpt-15\n\nEpoch 17/20\n\nEpoch 17 completed. Average loss: 0.0995\nCheckpoint saved: film_model_checkpoints/ckpt-16\n\nEpoch 18/20\n\nEpoch 18 completed. Average loss: 0.0979\nCheckpoint saved: film_model_checkpoints/ckpt-17\n\nEpoch 19/20\n\nEpoch 19 completed. Average loss: 0.0965\nCheckpoint saved: film_model_checkpoints/ckpt-18\n\nEpoch 20/20\n\nEpoch 20 completed. Average loss: 0.0940\nCheckpoint saved: film_model_checkpoints/ckpt-19\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nclass FILMInferencer:\n    def __init__(self, \n                 model_url: str = \"https://tfhub.dev/google/film/1\",\n                 checkpoint_dir: str = \"film_model_checkpoints\"):\n        \"\"\"\n        Initialize FILM model inferencer\n        \n        Args:\n            model_url: URL for the FILM model\n            checkpoint_dir: Directory containing saved checkpoints\n        \"\"\"\n        self.model = hub.load(model_url)\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoint = tf.train.Checkpoint(model=self.model)\n        self.manager = tf.train.CheckpointManager(self.checkpoint, checkpoint_dir, max_to_keep=5)\n\n        # Restore the latest checkpoint\n        if self.manager.latest_checkpoint:\n            self.checkpoint.restore(self.manager.latest_checkpoint).expect_partial()\n            print(f\"Restored model from checkpoint: {self.manager.latest_checkpoint}\")\n        else:\n            print(\"No checkpoint found. Using the pre-trained model from TensorFlow Hub.\")\n\n    def load_and_preprocess_image(self, image_path: str, image_size: tuple) -> tf.Tensor:\n        \"\"\"\n        Load and preprocess an image for inference\n        \n        Args:\n            image_path: Path to the image\n            image_size: Target image size (height, width)\n        \n        Returns:\n            Preprocessed image tensor\n        \"\"\"\n        # Load and decode the image\n        image = tf.io.read_file(image_path)\n        image = tf.io.decode_image(image, channels=3, dtype=tf.float32)\n        # Resize the image to the target size\n        image = tf.image.resize(image, image_size)\n        return image\n\n    def predict_interpolated_frame(self, image0: str, image1: str, image_size: tuple = (256, 256)) -> np.ndarray:\n        \"\"\"\n        Predict the interpolated frame between two images\n        \n        Args:\n            image0: Path to the first input image\n            image1: Path to the second input image\n            image_size: Target image size (height, width)\n        \n        Returns:\n            Interpolated frame as a NumPy array\n        \"\"\"\n        # Load and preprocess images\n        img0 = self.load_and_preprocess_image(image0, image_size)\n        img1 = self.load_and_preprocess_image(image1, image_size)\n        \n        # Add batch dimension\n        img0 = tf.expand_dims(img0, axis=0)\n        img1 = tf.expand_dims(img1, axis=0)\n        \n        # Time tensor for interpolation (always 0.5 for midpoint frame)\n        time = tf.convert_to_tensor([[0.5]], dtype=tf.float32)\n        \n        # Perform inference\n        inputs = {'time': time, 'x0': img0, 'x1': img1}\n        output_dict = self.model(inputs)\n        interpolated_frame = output_dict['image'][0].numpy()\n        \n        # Convert to uint8 and denormalize for visualization\n        interpolated_frame = (interpolated_frame * 255).astype(np.uint8)\n        return interpolated_frame\n\n# Function to visualize the images\ndef visualize_images(image0_path, image1_path, interpolated_frame, output_path=\"visualized_output.png\"):\n    \"\"\"\n    Save the visualization of two input images and the interpolated frame.\n\n    Args:\n        image0_path: Path to the first input image\n        image1_path: Path to the second input image\n        interpolated_frame: Interpolated frame as a NumPy array\n        output_path: Path to save the visualized output\n    \"\"\"\n    # Load input images\n    image0 = cv2.imread(image0_path)\n    image1 = cv2.imread(image1_path)\n\n    # Resize input images to match the interpolated frame size\n    height, width, _ = interpolated_frame.shape\n    image0 = cv2.resize(image0, (width, height))\n    image1 = cv2.resize(image1, (width, height))\n\n    # Convert BGR to RGB for consistency\n    image0 = cv2.cvtColor(image0, cv2.COLOR_BGR2RGB)\n    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n\n    # Stack the images horizontally\n    stacked_images = np.hstack((image0, interpolated_frame, image1))\n\n    # Convert RGB to BGR for saving\n    stacked_images_bgr = cv2.cvtColor(stacked_images, cv2.COLOR_RGB2BGR)\n\n    # Save the visualized output\n    cv2.imwrite(output_path, stacked_images_bgr)\n    print(f\"Visualization saved to: {output_path}\")\n\n\n\ndef main():\n    # Paths to the input images\n    image0_path = \"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset/sample_1/image.png\"\n    image1_path = \"/kaggle/working/SIH_UI_Geoserver/Frame_Interpolation/interpolation_dataset/sample_1/image2.png\"\n    \n    # Check if files exist\n    if not os.path.exists(image0_path) or not os.path.exists(image1_path):\n        raise FileNotFoundError(\"Input image files not found\")\n    \n    # Initialize FILM inferencer\n    inferencer = FILMInferencer()\n    \n    # Predict the interpolated frame\n    interpolated_frame = inferencer.predict_interpolated_frame(image0_path, image1_path)\n    print(\"Interpolated frame prediction completed\")\n    \n    # Save and visualize the result\n    output_path = \"interpolated_frame.png\"\n    cv2.imwrite(output_path, cv2.cvtColor(interpolated_frame, cv2.COLOR_RGB2BGR))\n    print(f\"Interpolated frame saved to: {output_path}\")\n    visualize_images(image0_path, image1_path, interpolated_frame, output_path=\"visualized_output.png\")\n    \n    # Visualize\n    # visualize_images(image0_path, image1_path, interpolated_frame)\n\nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T16:53:12.933812Z","iopub.execute_input":"2024-12-08T16:53:12.934295Z","iopub.status.idle":"2024-12-08T16:53:18.103849Z","shell.execute_reply.started":"2024-12-08T16:53:12.934260Z","shell.execute_reply":"2024-12-08T16:53:18.102915Z"}},"outputs":[{"name":"stdout","text":"Restored model from checkpoint: film_model_checkpoints/ckpt-19\nInterpolated frame prediction completed\nInterpolated frame saved to: interpolated_frame.png\nVisualization saved to: visualized_output.png\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}